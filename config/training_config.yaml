# Training Configuration

training:
  # Basic Settings
  num_epochs: 50
  batch_size: 16
  gradient_accumulation_steps: 1
  seed: 42
  device: "cuda"              # cuda, cpu, or mps
  mixed_precision: true       # Use FP16 mixed precision training
  
  # Optimization
  optimizer:
    type: "adamw"             # adamw, adam, sgd
    lr: 5.0e-4                # Learning rate
    weight_decay: 0.01        # Weight decay for AdamW
    betas: [0.9, 0.999]       # Adam betas
    eps: 1.0e-8               # Optimizer epsilon
    
  # Learning Rate Schedule
  lr_scheduler:
    type: "cosine"            # cosine, linear, step, plateau
    warmup_steps: 1000        # Number of warmup steps
    warmup_ratio: 0.1         # Warmup as ratio of total steps (alternative to warmup_steps)
    min_lr: 1.0e-6            # Minimum learning rate
    
  # Loss Configuration
  loss:
    # Hungarian Matching Loss
    use_hungarian: true
    cost_class: 1.0           # Classification cost weight in matching
    cost_amount: 5.0          # Amount regression cost weight in matching
    
    # Final Loss Weights
    loss_class: 1.0           # Cross-entropy loss weight
    loss_amount: 5.0          # L1 loss weight for amounts
    loss_balance: 2.0         # Debit-credit balance constraint weight
    
    # Class Imbalance Handling
    use_focal_loss: true
    focal_alpha: 0.25
    focal_gamma: 2.0
    no_entry_weight: 0.1      # Down-weight "no entry" class
    
    # Hierarchical Losses
    use_hierarchical_loss: true
    hierarchy_weights: [1.0, 0.5, 0.25, 0.1]  # Weights for each hierarchy level
    
  # Regularization
  dropout: 0.1
  label_smoothing: 0.0
  gradient_clip_norm: 1.0     # Gradient clipping
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 10              # Epochs to wait for improvement
    min_delta: 0.001          # Minimum change to qualify as improvement
    monitor: "val_loss"       # Metric to monitor
    
  # Checkpointing
  checkpoint:
    save_every_n_epochs: 5
    save_best_only: true
    monitor: "val_accuracy"   # Metric to monitor for best model
    mode: "max"               # max or min
    
  # Logging
  logging:
    log_every_n_steps: 50
    use_tensorboard: true
    use_wandb: false
    wandb_project: "journal-entry-prediction"
    wandb_entity: null
    
  # Validation
  validation:
    val_every_n_epochs: 1
    val_batch_size: 32
    
# Data Configuration (to be used with user's data loaders)
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  shuffle: true
  num_workers: 4
  pin_memory: true
  
# Curriculum Learning (optional)
curriculum:
  enabled: false
  start_with_simple: true
  difficulty_metric: "num_entry_lines"  # How to measure difficulty
  schedule: "linear"                     # linear, step, exponential

